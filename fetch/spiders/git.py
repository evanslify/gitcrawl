import scrapy
import ujson as json
import re
from fetch.items import GithubItem


class GitSpider(scrapy.Spider):
    name = 'github'
    http_user = 'evanslify'
    http_pass = '4e4b57cc169d0e7a6812f73a9a48843b84a2200b'

    def __init__(self, *args, **kwargs):
        super(GitSpider, self).__init__(*args, **kwargs)
        self.mode = kwargs.get('mode', 'all').split(',')
        self.target = kwargs.get('target', '')

    def callnext(self, response=None, caller=None,
                 html=False, start_meta=None):
        # trick to override the default parsing method :D
        if start_meta:
            meta = start_meta
        elif not start_meta and response:
            meta = response.request.meta
        else:
            raise Exception

        callstack = meta['callstack']

        if html is False and response is not None:
            link = response.headers.get('Link')
            if link is not None:
                url = self.page_plus_one(link, response.url)
                if url is not None:
                    callstack.append({
                        'url': url, 'callback': caller
                    })

        if len(callstack) > 0:
            target = callstack.pop(0)
            url = target['url']
            # automatically add this string to url,
            # so that page_plus_one will work
            if 'per_page=' and 'page=' not in url:
                url = url + '?per_page=100&page=1'
            yield scrapy.Request(
                url=url, meta=meta,
                callback=target['callback'], errback=self.callnext)

        else:
            items = GithubItem()
            loader = response.meta.get('Loader')
            for key in loader.iterkeys():
                items[key] = loader[key]
            yield items

    def page_plus_one(self, header, url):
        # called only when response.headers.get('Link') returns valid

        if header.find('next') != -1:
            # if valid, first url will be the next page
            new_page_number = re.findall('(?<=&page=)\d+', header)[0]
            new_page = str(re.sub('(?<=&page=)\d+', new_page_number, url))
        else:
            new_page = None
        return new_page

    def start_requests(self):

        meta = scrapy.Request.meta
        # declaring item loader's layout.
        meta = {
            'callstack': [],
            'Loader': {
                'UserInfo': {},
                'GistInfo': [],
                'RepoInfo': {},
                'identifier': self.target,
            }
        }
        callstack = meta['callstack']
        calls = self.parse_arguments()
        callstack.extend(calls)
        return self.callnext(start_meta=meta)

    def parse_arguments(self):
        # returns a list, generated by parsing start arguments
        # which can be extended into callstack
        # raises exception while mode is invalid

        actions = []
        base_url = 'https://api.github.com'
        user_url = base_url + '/users/%s' % self.target
        gist_url = user_url + '/gists'
        repo_url = user_url + '/repos'

        url_dict = {
            'user': (user_url, self.crawl_user_page),
            'gist': (gist_url, self.crawl_user_gist),
            'repo': (repo_url, self.crawl_user_repo)
        }
        if 'all' in self.mode:
            for value in url_dict.itervalues():
                actions.append({
                    'url': value[0], 'callback': value[1]
                })
        else:
            for mode in self.mode:
                if mode in url_dict.iterkeys():
                    detail = url_dict[mode]
                    actions.append({
                        'url': detail[0], 'callback': detail[1]
                    })
                else:
                    raise Exception('Parsing mode invalid.')
        return actions
# ----------------------------------------------------------------
# Start declaring methods to parse JSON items.
# ----------------------------------------------------------------

    def parse_mini_repo(self, input_):
        result = {
            'repo_id': input_.get('id'),
            'repo_name': input_.get('name')
        }
        return result

# ------- functions for parsing events.
    def parse_milestone(self, input_):
        creator = input_.get('creator')
        creator_info = self.parse_mini_user(creator)
        result = {
            'title': input_.get('title'),
            'description': input_.get('description'),
            'open_issues': input_.get('open_issues'),
            'closed_issues': input_.get('closed_issues'),
            'state': input_.get('state'),
            'created_at': input_.get('created_at'),
            'updated_at': input_.get('updated_at'),
            'creator': creator_info
        }
        return result

    def parse_issue_label(self, input_):
        result = {
            'name': input_.get('name'),
            'color': input_.get('color')
        }
        return result

    def parse_event_payload_issue(self, input_):

        result = {
            'created_at': input_.get('created_at'),
            'updated_at': input_.get('updated_at'),
            'closed_at': input_.get('closed_at'),
            'comments': input_.get('comments'),
            'issue_id': input_.get('id'),
            'number': input_.get('number'),
            'title': input_.get('title'),
            'body': input_.get('body'),
            'action': input_.get('action')
        }

        user = input_.get('user')
        if user:
            user_info = self.parse_mini_user(user)
            result['user'] = user_info

        assignee = input_.get('assignee')
        if assignee:
            assignee_info = self.parse_mini_user(assignee)
            result['assignee'] = assignee_info

        labels = input_.get('labels')
        if labels:
            for i in labels:
                labels_info = self.parse_issue_label(i)
            result['labels'] = labels_info

        return result

    def parse_push_commits(self, input_):
        result = {
            'sha': input_.get('sha'),
            'author': input_.get('author'),
            'message': input_.get('message'),
            'distinct': input_.get('distinct'),
            'url': input_.get('url')
        }
        return result

    def parse_event_payload_push(self, input_):
        commits = input_.get('commits')
        commits_list = []
        for i in commits:
            info = self.parse_push_commits(i)
            commits_list.append(info)

        result = {
            'push_id': input_.get('push_id'),
            'size': input_.get('size'),
            'disinct_size': input_.get('disinct_size'),
            'ref': input_.get('ref'),
            'head_after': input_.get('head'),
            'head_before': input_.get('before'),
            'commits': commits_list,
            'created_at': input_.get('created_at')
        }
        return result

    def parse_event(self, input_):

        event_type = input_['type']
        payload = input_['payload']
        if event_type == 'PushEvent':
            payload_info = self.parse_event_payload_push(payload)
        elif event_type == 'IssuesEvent':
            payload_info = self.parse_event_payload_issue(payload)

        repo = input_['repo']
        repo_info = self.parse_mini_user(repo)
        actor = input_['actor']
        actor_info = self.parse_mini_user(actor)

        org = input_.get('org')
        if org:
            org_info = self.parse_mini_user(org)
        else:
            org_info = None

        result = {
            'event_id': input_['id'],
            'type': event_type,
            'created_at': input_['created_at'],
            'repo': repo_info,
            'org': org_info,
            'payload': payload_info,
            'actor': actor_info
        }
        return result

# ------- functions for parsing events.

    def parse_user(self, input_):
        item = {
            'user_html_url': input_.get('html_url'),
            'user_public_repo_count': input_.get('public_repos'),
            'user_public_gist_count': input_.get('public_gists'),
            'user_email': input_.get('email'),
            'user_followers_count': input_.get('followers'),
            'user_company': input_.get('compant'),
            'user_hireable': input_.get('hireable'),
            'user_id': input_.get('id'),
            'user_login': input_.get('login'),
            'user_display_name': input_.get('name'),
            'user_blog': input_.get('blog'),
            'user_location': input_.get('location'),
            'user_bio': input_.get('bio'),
            'user_following_count': input_.get('following'),
            'user_created': input_.get('created_at'),
            'user_updated': input_.get('updated_at'),
            'user_url': input_.get('url')
        }
        # item = dict((k, v) for k, v in item.iteritems() if v is not None)

        return item

    def parse_mini_user(self, input_):
        # parses mini user object
        result = {
            'user_id': input_.get('id'),
            'user_login': input_.get('login')
        }
        return result

    def parse_contributor(self, input_):
        # parses contributor
        result = {
            'user_id': input_.get('id'),
            'user_login': input_.get('login'),
            'times_contributed': input_.get('contributions')
        }
        return result

    def parse_gist(self, input_):
        # parses one gist object, returns a 2-list:
        # list[0]: gist object ; list[1]: [gist comment URL, gist id]
        files = input_['files'].values()

        file_info = []
        for i in files:
            info = {
                'type': i.get('type'),
                'name': i.get('filename'),
                'raw_url': i.get('raw_url'),
                'language': i.get('language')
            }
        file_info.append(info)

        comments = input_.get('comments')
        comment_url = input_.get('comments_url') if comments > 0 else None

        gist = {
            'gist_id': input_.get('id'),
            'gist_created': input_.get('created_at'),
            'gist_comment_counts': comments,
            'gist_comment_url': comment_url,
            'gist_file_info': file_info
        }
        gist = dict((k, v) for k, v in gist.iteritems() if v is not None)
        result = [gist, comment_url]
        return result

    def parse_gist_comment(self, input_):
        user = input_['user']
        user_info = {
            'user_id': user.get('id'),
            'user_login': user.get('login')
        }
        result = {
            'time': input_.get('created_at'),
            'content': input_.get('body'),
            'comment_id': input_.get('id'),
            'user': user_info
        }
        return result

    def parse_repository(self, input_, detailing=False):
        # returns list, [result, status]
        forked = input_['fork']
        stargazers_count = input_['stargazers_count']
        forks_count = input_['forks_count']
        open_issues_count = input_['open_issues_count']
        html_url = input_['html_url']
        status = {
            'fork': forked,
            'starred': bool(stargazers_count),
            'forks': bool(forks_count),
            'open_issues': bool(open_issues_count)
        }
        extras_url = {
            'stargazers_url': input_['stargazers_url'],
            'contributors_url': input_['contributors_url']
        }

        repo = {
            'id': input_['id'],
            'name': input_['name'],
            'created_at': input_['created_at'],
            'updated_at': input_['updated_at'],
            'pushed_at': input_['pushed_at'],
            'homepage': input_['homepage'],
            'stargazers_count': stargazers_count,
            'watchers_count': input_['watchers_count'],
            'language': input_['language'],
            'forks_count': forks_count,
            'forked': forked,
            'open_issues_count': open_issues_count,
            'html_url': html_url,
            'zipurl': html_url + "/archive/master.zip",
            'url': input_['url'],
            'description': input_['description'],
            'extras_url': extras_url
        }

        if detailing is True:
            owner = input_.pop('owner')
            owner_info = self.parse_mini_user(owner)
            repo.update({
                owner: owner_info
            })
        result = [repo, status]
        return result
# ----------------------------------------------------------------
# End declaring methods to parse JSON items.
# ----------------------------------------------------------------

    def crawl_user_page(self, response):
        # crawls user page.
        # https://api.github.com/users/<username>

        jr = json.loads(response.body_as_unicode())
        callstack = response.meta['callstack']
        loader = response.meta['Loader']

        user_item = self.parse_user(jr)
        loader['UserInfo'] = user_item

        html_url = user_item['user_html_url']
        followers_count = user_item['user_followers_count']
        following_count = user_item['user_following_count']
        url = user_item['user_url']

        html_followers = html_url + '/followers'
        html_following = html_url + '/following'
        html_userpage_js = (
            html_url + '?tab=contributions&from=2013-01-08'
            '&_pjax=.js-contribution-activity')

        followers_url = url + '/followers'
        following_url = url + '/following'
        starred_url = url + '/starred'
        events_url = url + '/events'

        actions = []
        if followers_count > 0:
            actions.extend([
                {
                    'url': followers_url,
                    'callback': self.crawl_user_fellows
                },
                {
                    'url': html_followers,
                    'callback': self.crawl_html_user_fellow
                }])

        if following_count > 0:
            actions.extend([
                {
                    'url': following_url,
                    'callback': self.crawl_user_fellows
                },
                {
                    'url': html_following,
                    'callback': self.crawl_html_user_fellow
                }])

        actions.extend([
            {'url': starred_url, 'callback': self.crawl_user_starred},
            {'url': html_userpage_js, 'callback': self.crawl_html_userpage_js},
            {'url': events_url, 'callback': self.crawl_user_events}])
        callstack.extend(actions)

        return self.callnext(response)

    def crawl_html_userpage_js(self, response):
        loader = response.meta['Loader']
        items = loader['UserInfo']

        contrib_number = response.selector.xpath(
            '//span[@class="contrib-number"]/text()').extract()
        # Sep 9: If user is Group this will still run
        # Monkey patch first
        try:
            contrib_number[0]
        except IndexError:
            return self.callnext(response)

        items.update({
            'user_last_year_contributes': contrib_number[0],
            'user_longest_streak': contrib_number[1],
            'user_current_streak': contrib_number[2],
        })

        return self.callnext(response)

    def crawl_html_user_fellow(self, response, mode=None):
        # crawls followers / following at HTML

        callstack = response.meta['callstack']
        loader = response.meta['Loader']

        if 'following' in response.url:
            items = loader['UserInfo'].setdefault('user_following', [])
        elif 'followers' in response.url:
            items = loader['UserInfo'].setdefault('user_followers', [])
        else:
            raise Exception

        name_list = response.selector.xpath(
            '//h3[@class="follow-list-name"]/span/a/text()').extract()
        info_list = response.selector.xpath(
            '//p[@class="follow-list-info"]/descendant-or-self::text()'
        ).extract()
        info_list = [name for name in info_list if name.strip()]

        for name in range(0, len(name_list)):
            username = response.selector.xpath(
                '//h3[@class="follow-list-name"]/span/a/@href'
            ).extract()[name][1:]
            # name shall do this in a more elegant way.
            # start
            try:
                user = filter(lambda x: x['user_login'] == username, items)[0]
            except IndexError:
                items.append({
                    'user_login': username
                })
                user = filter(lambda x: x['user_login'] == username, items)[0]

            user.update({
                'user_display_name': name_list[name],
                'user_info': info_list[name],
            })
            # end
        html_pagination = response.selector.xpath(
            '//div[@class="pagination"]/a[text()[contains(.,"Next")]]/@href'
        ).extract()

        if len(html_pagination) > 0:
            callstack.append({
                'url': html_pagination[0],
                'callback': self.crawl_html_user_fellow})

        return self.callnext(response, html=True)

    def crawl_user_fellows(self, response):
        # crawls followers / following

        jr = json.loads(response.body_as_unicode())
        loader = response.meta['Loader']
        items = loader['UserInfo']

        if 'followers' in response.url:
            key_name = 'user_followers'
        elif 'following' in response.url:
            key_name = 'user_following'
        else:
            raise Exception

        user_list = items.setdefault(key_name, [])

        for mini_user in jr:
            user = self.parse_mini_user(mini_user)
            user_list.append(user)

        return self.callnext(response, caller=self.crawl_user_fellows)

    def crawl_user_gist(self, response):
        # only called when public_gists > 0
        # https://api.github.com/users/<username>/gists

        jr = json.loads(response.body_as_unicode())
        callstack = response.meta['callstack']
        loader = response.meta['Loader']
        items = loader['GistInfo']

        for one_gist in jr:
            gist = self.parse_gist(one_gist)
            if gist[1] is not None:
                url = gist[1]
                callstack.insert(0, {
                    'url': url, 'callback': self.crawl_user_gist_comments
                })
            items.append(gist[0])
        return self.callnext(response, caller=self.crawl_user_gist)

    def crawl_user_gist_comments(self, response):
        # only called when a gist has comment.
        # https://api.github.com/gists/<Gist ID>/comments

        jr = json.loads(response.body_as_unicode())
        loader = response.meta['Loader']
        items = loader['GistInfo']
        gist_id = response.url.split('/')[-2]
        target_gist = filter(lambda x: x['gist_id'] == gist_id, items)[0]

        comments = []
        for one_comment in jr:
            comment = self.parse_gist_comment(one_comment)
            comments.append(comment)
        target_gist['comments'] = comments

        return self.callnext(response, caller=self.crawl_user_gist_comments)

    def crawl_user_starred(self, response):
        # always called. make sure to check whether len(jr) is true.
        # https://api.github.com/users/<username>/starred

        jr = json.loads(response.body_as_unicode())
        loader = response.meta['Loader']
        items = loader['UserInfo']

        if len(jr) > 0:
            starred = []
            for i in jr:
                info = self.parse_repository(i)[0]
                starred.append(info)
            items['user_starred'] = starred
        else:
            pass
        return self.callnext(response, caller=self.crawl_user_starred)

    def crawl_user_repo(self, response):
        # https://api.github.com/users/<username>/repos
        # parsing repository list of a user
        # includes repo info

        jr = json.loads(response.body_as_unicode())
        callstack = response.meta['callstack']
        loader = response.meta['Loader']
        items = loader['RepoInfo']

        for repo in jr:
            result = self.parse_repository(repo)
            repo_info = result[0]
            repo_name = repo_info['name']
            repo_extra_url = repo_info['extras_url']
            status = result[1]
            items.update({
                repo_name: repo_info
            })
            action = []
            for status_value in status:
                if 'forked' == True:
                    repo_url = repo_info.get('url')
                    action.append({
                        'url': repo_url,
                        'callback': self.crawl_repo_detail
                    })
                if 'starred' == True:
                    stargazer_url = repo_extra_url.get('stargazers_url')
                    action.append({
                        'url': stargazer_url,
                        'callback': self.crawl_repo_stars
                    })
                if 'forks' == True:
                    forks_url = repo_info.get('forks_url')
                    action.append({
                        'url': forks_url,
                        'callback': self.crawl_repo_forks
                    })
                if 'open_issues' == True:
                    issues_url = repo_info.get('url') + '/issues'
                    action.append({
                        'url': issues_url,
                        'callback': self.crawl_repo_issues
                    })
            contributors_url = repo_extra_url.get('contributors_url')
            action.append({
                'url': contributors_url,
                'callback': self.crawl_repo_contributors
            })
            callstack.extend(action)

        return self.callnext(response, caller=self.crawl_user_repo)

    def crawl_repo_forks(self, response):
        # parse repo forks
        # this is loaded only while forked count > 1
        # https://api.github.com/repos/<username>/<repository name>/forks

        loader = response.meta['Loader']
        items = loader['RepoInfo']

        repo_name = response.url.split('/')[-2]
        item = items[repo_name].setdefault('forks', [])
        jr = json.loads(response.body_as_unicode())

        for forking_repo in jr:
            result = self.parse_repository(forking_repo, detailing=True)
            item.append(result)

        return self.callnext(response, caller=self.crawl_repo_forks)

    def crawl_repo_contributors(self, response):
        # parse this repo's count of contributors; always runned
        loader = response.meta['Loader']
        repo_name = response.url.split('/')[-2]
        item = loader['RepoInfo'][repo_name]

        if response.status == 204:
            return self.callnext(response)
        else:
            jr = json.loads(response.body_as_unicode())

        times_contributed = len(jr)
        item['times_contributed'] = times_contributed

        # check whether this repo has contributors
        if times_contributed > 0:
            contributors = item.setdefault('contributed_users', [])
            for contributing_user in jr:
                result = self.parse_contributor(contributing_user)
                contributors.append(result)
        return self.callnext(response, caller=self.crawl_repo_contributors)

    def crawl_repo_detail(self, response):
        # crawls detail page of a repository.
        # only called while a repository is forked from someone.
        loader = response.meta['Loader']
        repo_name = response.url.split('/')[-1]
        items = loader['RepoInfo'][repo_name]
        jr = json.loads(response.body_as_unicode())

        parent = jr.get('parent')
        parent_info = self.parse_repository(parent, detailing=True)
        source = jr.get('source')
        source_info = self.parse_repository(source, detailing=True)
        result = {
            'fork_parent': parent_info,
            'fork_source': source_info
        }
        items['forkinfo'] = result

        return self.callnext(response, caller=self.crawl_repo_detail)

    def crawl_repo_stars(self, response):
        # https://api.github.com/repos/<username>/<repository name>/stargazers
        # only called while a repository has stargazers.
        loader = response.meta['Loader']
        repo_name = response.url.split('/')[-1]
        item = loader['RepoInfo'][repo_name].setdefault('stargazers', [])
        jr = json.loads(response.body_as_unicode())

        for starring_user in jr:
            result = self.parse_mini_user(starring_user)
            item.append(result)

        return self.callnext(response, caller=self.crawl_repo_stars)

    def crawl_user_events(self, response):

        jr = json.loads(response.body_as_unicode())
        loader = response.meta['Loader']
        items = loader['UserInfo'].setdefault('user_events', [])

        allowed = ['PushEvent', 'IssuesEvent']

        for event in jr:
            event_type = event['type']
            if event_type in allowed:
                event = self.parse_event(event)
                items.append(event)

        return self.callnext(response, caller=self.crawl_user_events)
